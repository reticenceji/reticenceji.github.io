---
aliases: 
tags: 
date_modified: 2024-12-11
date: 2024-12-04
---

# 统计概念

一些统计的概念常常被用在机器学习中。

## 平方误差

平方误差 (Squared Error)是模型预测值与实际观测值之间的差异的平方。公式为：

$$SE_i=(y_i−\hat y_i)^2$$

其中，$y_i$是第$i$个样本的实际观测值，$\hat{y}_i$是模型对该样本的预测值。

均方误差（Mean Square Error，MSE）又称为二次损失、L2损失，就是所有样本平方误差的平均值，常用于**回归**问题中。

假设有 $n$ 个训练数据，则均方误差为：

$$MSE=\frac{1}{n}\sum_{i=1}^n(y_i−\hat y_i)^2$$

## 基尼指数

基尼指数（Gini Index）：假设数据集$D$中有$K$个类，样本属于第$k$类的比例为$p_k$，基尼指数$Gini(D)$ 表示集合D的不确定性（纯度），公式为：

$$Gini(D) = \sum^K_{k=1}p_k(1-p_k)=1-\sum^K_{k=1}p_k^2$$ 

如何理解这个公式？如果所有样品属于同一类别，则基尼指数为0，表示完全纯净。如果类别分布均匀（每个类别的样本比例相同），则基尼不纯度达到最大值。

对于特征$A$，将集合$D$划分成$D_1$和$D_2$，基尼指数$Gini(D,A)$表示经过$A=a$划分后集合$D$的不确定性，公式为：

$$Gini(D,A=a)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$ 

更一般的，如果特征$T$将集合$D$划分成$K$个类：

$$Gini(D,T)=\sum_{k=1}^K \frac{|D_k|}{|D|}Gini(D_k)$$

在分类任务中，我们想要找的就是一个特征$A=a$，让上面的公式尽可能的小，说明这个特征非常的有效，分的泾渭分明。

> 你可能会想到用来描述贫富差距的基尼系数（Gini Coefficient），他们的关系不大。

## 熵

在物理学中，“熵”被用来表示热力学系统所呈现的无序程度。香农将这一概念引入信息论领域，提出了“信息熵”概念，通过对数函数来测量信息的不确定性。

## 残差

在回归分析中，残差是观测值与预测值之间的差异。简单来说，如果你有一个预测模型，残差就是**实际数据点与模型预测值之间的距离**。
